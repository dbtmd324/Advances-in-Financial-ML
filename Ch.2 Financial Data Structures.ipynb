{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch.2 Financial Data Structures\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Motivation\n",
    "\n",
    "We will learn how to work with unstructured financial data, and from that to derive a structured dataset amenable to ML algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Essential Types of Financial Data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fundamental Data</th>\n",
       "      <th>Market Data</th>\n",
       "      <th>Analytics</th>\n",
       "      <th>Alternative Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assets</td>\n",
       "      <td>Price/Yield/Implied volatility</td>\n",
       "      <td>Analyst recommendations</td>\n",
       "      <td>Satellite/CCTV images</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Liabilities</td>\n",
       "      <td>Volume</td>\n",
       "      <td>Credit ratings</td>\n",
       "      <td>Google searches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sales</td>\n",
       "      <td>Dividend/coupons</td>\n",
       "      <td>Earnings expectation</td>\n",
       "      <td>Twiiter/chats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Costs/earnings</td>\n",
       "      <td>Open interest</td>\n",
       "      <td>News sentiment</td>\n",
       "      <td>Metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Macro variables</td>\n",
       "      <td>Qouotes/cancellations</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Fundamental Data                     Market Data                Analytics  \\\n",
       "0           Assets  Price/Yield/Implied volatility  Analyst recommendations   \n",
       "1      Liabilities                          Volume           Credit ratings   \n",
       "2            Sales                Dividend/coupons     Earnings expectation   \n",
       "3   Costs/earnings                   Open interest           News sentiment   \n",
       "4  Macro variables           Qouotes/cancellations                        0   \n",
       "\n",
       "        Alternative Data  \n",
       "0  Satellite/CCTV images  \n",
       "1        Google searches  \n",
       "2          Twiiter/chats  \n",
       "3               Metadata  \n",
       "4                      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({'Fundamental Data': ['Assets', 'Liabilities', 'Sales', 'Costs/earnings', 'Macro variables'],\n",
    "              'Market Data': ['Price/Yield/Implied volatility', 'Volume', 'Dividend/coupons', 'Open interest', 'Qouotes/cancellations'], \n",
    "              'Analytics': ['Analyst recommendations', 'Credit ratings', 'Earnings expectation', 'News sentiment', '0'], \n",
    "              'Alternative Data': ['Satellite/CCTV images', 'Google searches', 'Twiiter/chats', 'Metadata', '0']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Fundamental Data\n",
    "\n",
    "Information that can be found in regulatory filings and business analytics. Once you align the data correctly, a substantial number of findings in those papers cannot be reproduced.\n",
    "- Mostly accounting data \n",
    "- Reported with a lapse\n",
    "- Bloomberg\n",
    "\n",
    "A second aspect of fundamental data is that it is often backfilled or reinstated.\n",
    "- Backfilling: Missing data is assigned a value, even ifthose values were unknown at that time.\n",
    "- Reinstated Value: A corrected value that amends an incorrect initial release.\n",
    "- The corrected values were not known on that first release date.\n",
    "\n",
    "Fundamental data is extremely regularized and low frequency. Being so accessible to the marketplace, it is rather unlikely that there is much value left to be exploited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Market Data\n",
    "\n",
    "Market data includes all trading activity that takes place in an exchange or trading venue. Every market participant leaves a characteristic footprint in the trading records, and with enough patience, you will find a way to anticipate a competitor`s next move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Analytics\n",
    "What characterizes analytics is not the content of the information, but that it is not readily available from an original source, and that it has been processed for you in a particular way. Investment banks and research firms sell valuable information that results from in-depths anlysis of companies` business  models, activities, competition, outlook, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Alternative Data\n",
    "\n",
    "It is primary information, that has not made it to the other sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Bars\n",
    "\n",
    "---\n",
    "\n",
    "1. Parse it\n",
    "2. Extract valuable information from it\n",
    "3. Store those extraction in a regularized format\n",
    "\n",
    "\n",
    "Most ML algorithms assume a table representation of the extracted data.\n",
    "- Finance practitioners often refer to those tables` rows as 'bars'.\n",
    "- Standard Bars / Information-Driven Bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Standard Bars\n",
    "\n",
    "To transform a series of observations that arrive at irregular frequency(often referred to as \"inhomogeneous series\") into a homogeneous series derived from regular sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.1 Time Bars (시간)\n",
    "\n",
    "Sampling information at fixed time intervals.\n",
    "- Timestamp\n",
    "- VWAP\n",
    "- OHLCV\n",
    "\n",
    "**Weakness**\n",
    "1. Markets do not process information at a constant time interval.\n",
    "   - Algorithms: Times bars oversample information during low-activity periods and undersample information during high-activity periods.\n",
    "   \n",
    "2. Time-sampled series often exhibit poor statistical porperties, like serieal correlation, heteroscadasticity, and non-noramlity of returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.2 Tick Bars (거래횟수)\n",
    "\n",
    "Sampling as a function of the number of transactions exhibited desirable statistical properties. \n",
    "- Gaussian distribution\n",
    "- Outliers\n",
    "- Closer to IID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.3 Volume Bars (거래량)\n",
    "\n",
    "One problem with tick bars is that order fragmentation introduces some arbitrariness in the number of ticks. \n",
    "- Volume bars circumvent that problem by sampling every time a pre-defined amount of the securitiy`s units(shares, futures contracts, etc) havebeen exhanged.\n",
    "- Sampling returns by volume achieved even better statistical properties(i.e. closer to an IID Gaussian distribution) than sampling by tick bars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.4 Dollar Bars (가격 변동)\n",
    "\n",
    "Sampling an observation every time a pre-defined market value is exchanged.\n",
    "- The number of shares traded is a function of the actual value exchanged.\n",
    "- The number of outstanding shares often changes multiple times over the course of a securities life, as a result of corporate actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Information-Driven Bars\n",
    "\n",
    "The purpose of information-driven bars is to sample more frequently when new information arrives to the market. By synchronizing sampling with the arrival of informed traders, we may be able to make decisions before prices reach a new equilibrium level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2.1 Tick Imbalance Bars\n",
    "\n",
    "Consider a sequence of ticks ${{(P_{t}, V_{t})}}_{t=1}^T$ where $P_t$ is the price with tick t, $V_t$ is the volume with tick t.\n",
    "Tick rule defines a sequence $({b_t})_{t=1}^T$ where \n",
    "\n",
    "$$b_t = \\begin{cases}\n",
    "b_{t-1}, & \\text{if }\\Delta p_t = 0 \\\\\n",
    "\\frac{|\\Delta p_t|} {\\Delta p_t}, & \\text{if } \\Delta p_t \\neq 0\n",
    "\\end{cases}$$ \n",
    "\n",
    "with $b_t \\in \\{-1, 1\\}$, and the boundary condition $b_0 = b_T$. The idea behind TIBs is to sample bars whenever tick imbalances exceed our expectations.\n",
    "- We wish to determine the tick index, T., such that the accumulation of signed ticks(signed according to the tick rule) exceeds a given threshold. \n",
    "\n",
    "1. Define the tick imbalance at time T as \n",
    "\n",
    "$$\\theta_T = \\Sigma_{t=1}^T b_t$$\n",
    "\n",
    "2. Compute the expected value of $\\theta_T$ at the beginning of the bar, \n",
    "\n",
    "   $$E_0[\\theta_T] = E_0[T](P[b_t=1]-P[b_t=-1])$$ \n",
    "   \n",
    "   where $E_0[T]$ is the expected size of the tick bar, $P[b_t=1]$ is the unconditional probability that a tick is classified as a buy, $P[b_t=-1]$ is the unconditional probability that a tick is classified as a sell.\n",
    "   - $E_0[\\theta_T]=E_0[T](2P[b_t=1]-1)$  \n",
    "\n",
    "\n",
    "3. Define TIB as a $T^*$ - contiguous subset of ticks such that the following condition is met: \n",
    " \n",
    " $$T^*= \\underset{T}{\\operatorname{argmin}} \\{|\\theta_T| \\ge E_0[T]|2P[b_t=1]-1|\\}$$\n",
    " \n",
    " where $|2P[b_t=1]-1|$ implies the size of the expected imbalance.\n",
    " - TIBs are produced more frequently under the presence of informed trading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2.2 Volue/Dollar Imbalance Bars\n",
    "\n",
    "The idea behind VIBS and DIBs is to extend the concept of TIBs.\n",
    "- Sample bars when volume or dollar imbalances diverge from our expectations.\n",
    "\n",
    "1. Define the imbalance at time T as \n",
    "\n",
    "   $$\\theta_T = \\Sigma_{t=1}^T b_t v_t$$ \n",
    "\n",
    "   where $v_t$ is the number of securities traded(VIB) or the dollar amount exchanged(DIB).\n",
    "   \n",
    "\n",
    "\n",
    "2. Compute the expected value of $\\theta_T$ at the beginning of the bar,\n",
    "\n",
    "   $$E_0[\\theta_T]=E_0[\\Sigma_{t|b_t=1}^T v_t]-[\\Sigma_{t|b_t=-1}^T v_t]=E_0[T](P[b_t=1] E_0[v_t|b_t=1]-P[b_t=-1] E_0[v_t|b_t=-1])$$\n",
    "   \n",
    "   $v^+=P[b_t=1]E_0[v_t|b_t=1]$, $v^-=P[b_t=-1]E_0[v_t|b_t=-1]$, $v^+ + v^-=E_0[T]^{-1} E_0[\\Sigma_t v_t]=E_0[v_t]$, then\n",
    "   - $E_0[\\theta_T]=E_0[T](v^+ - v^-)=E_0[T](2v^+ - E_0[v_t])$\n",
    "\n",
    "\n",
    "\n",
    "3. Define VIB or DIB as a $T^*$ - contiguous subset of ticks such that the following condition is met: \n",
    "\n",
    "   $$T^*= \\underset{T}{\\operatorname{argmin}} \\{|\\theta_T| \\ge E_0[T]|2v^+ - E_0[v_t]|\\}$$\n",
    "   \n",
    "   - When $\\theta_T$ is more imbalanced thatn expected, a low T will satisfy these conditions.\n",
    "   - Tick fragmentation & Outliers\n",
    "   - The issue of corporate actions\n",
    "   - The bar size is adjusted dynamically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2.3 Tick Runs Bars\n",
    "\n",
    "Large traders will sweep the order book, use iceberg orders, or slice  a parent order into multiple children, all of which leave a trace of runs in the $\\{b_t\\}_{t=1...T}$ sequence. \n",
    "- It can be useful to monitor the sequence of buys in the overall volume, and take samples when that sequence diverges from our expectationss.\n",
    "\n",
    "1. Define the length of the current run as \n",
    "\n",
    "   $$\\theta_T=max\\{\\Sigma_{t|b_t=1}^T b_t, -\\Sigma_{t|b_t=-1}^T b_t \\}$$\n",
    "   \n",
    "2. Compute the expected value of $\\theta_T$ at the beginning of the bar,\n",
    "\n",
    "   $$E_0[\\theta_T]=E_0[T]max\\{P[B_t=1], 1-P[b_t=1] \\}$$  \n",
    "\n",
    "\n",
    "3. Define a tick runs bar (TRB) as a $T^*$ - contiguous subset of ticks such that the following condition is met:\n",
    "\n",
    "\n",
    "   $$T^*= \\underset{T}{\\operatorname{argmin}} \\{\\theta_T \\ge max\\{P[B_t=1], 1-P[b_t=1] \\}\\}$$\n",
    "   \n",
    "   where expected count of ticks from runs is implied by $max\\{P[B_t=1], 1-P[b_t=1] \\}$.\n",
    "   - When $\\theta_T$ exhibits more runs than expected, a low T will satisfy these conditions.\n",
    "   - Sequential Breaks: Instead of measuring the length of the longest sequence, we count the number of ticks of each side, without offering them(no imbalance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2.4 Volume / Dollar Runs Bars\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition is that we wish to sample bars whenever the volumes or dollars traded by one side exceed our expectation for a bar. \n",
    "\n",
    "1. Define the volumes or dollars associated with a run as\n",
    "\n",
    "   $$\\theta_T=max\\{\\Sigma_{t|b_t=1}^T b_t v_t, -\\Sigma_{t|b_t=-1}^T b_t v_t \\}$$\n",
    "   \n",
    "   where $v_t$: Number of securities traded(VRB) or dollar amount exchanged(DRB). \n",
    "   \n",
    "\n",
    "2. Compute the expected value of $\\theta_T\\$ at the beginning of the bar.\n",
    "\n",
    "   $$E_0 [\\theta_T] = E_0 [T] max\\{P[b_t=1] E_0 [v_t|b_t =1], (1-P[b_t=1]) E_0[v_t|b_t = -1] \\}$$  \n",
    "   \n",
    "   \n",
    "3. Define a VRB as a $T^*$ - contiguous subset of ticks such that the following condition is met:\n",
    "\n",
    "   \n",
    "   $$T^*= \\underset{T}{\\operatorname{argmin}} \\{\\theta_T \\ge E_0 [T] max\\{P[B_t=1] E_0[v_t|b_t =1], (1-P[b_t=1]) E_0 [v_t|b_t =-1] \\}\\}$$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Dealing with Multi-Product Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we are interested in modelling a time series of instruments, where the weights need to be dynamically adjusted over time.\n",
    "- If not, we will inadvertently introduce a structural break that will mislead our research efforts.\n",
    "- \"ETF Trick\": The goal is to transform any complex multi-product dataset into a single dataset that resembles a total-return ETF.\n",
    "- Because your code can always assume that you only trade cashlile prodcuts(non-expiring cash instruments), regardless of the complexity and composition of the underlying series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 The ETF Trick\n",
    "\n",
    "Suppose developing a strategy that trades a spread of futures.\n",
    "\n",
    "1. The spread is characterized by a vector of weights that changes over time.\n",
    "   - The spread itself may converge even if prices do not change.\n",
    "   - A model will be misled to believe that PnL has resulted from that weight-induced convergence. \n",
    "   \n",
    "   \n",
    "2. Spreads can acquire negative values, because they do not represent a price.\n",
    "\n",
    "\n",
    "3. Trading times will not align exactly for all constituents, so the spread is not always tradeable at the last levels published, or with zero latency risk. \n",
    "\n",
    "\n",
    "4. Execution costs must be considered, like crossing the bid-ask spread. \n",
    "\n",
    "\n",
    "One way to avoid these issues is to produce a time series that reflects the value of $1 invested in a spread.\n",
    "Suppose that we are given a history of bars, containing the following columns:\n",
    "\n",
    "- $0_{i,t}$ : The raw open price of instrument $i=1, ... , I$ at bar $t=1, ... , T$\n",
    "- $p_{i,t}$ : The raw close price of instrument $i=1, ... , I$ at bar $t=1, ... , T$\n",
    "- $\\psi_{i,t}$ : The USD value of one point $i=1, ... , I$ at bar $t=1, ... , T$ (includes foreign exchange rate)\n",
    "- $v_{i,t}$ : The volume of instrument $i=1, ... , I$ at bar $t=1, ... , T$\n",
    "- $d_{i,t}$ : The carry, dividend, or coupon paid by instrument $i=1, ... , I$ at bar $t=1, ... , T$ (Used to charge margin costs, or costs of funding)\n",
    "\n",
    "For a basket of futures characterized by an allocations vector $w_t$ rebalanced(or rolled) on bars $B \\subseteq {1,...,T}$, the 1 dollar investment value ${K_t}$ is desired as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_{i,t} = \\begin{cases}\n",
    "(w_{i,t} K_t) \\over (o_{i,t+1} \\psi_{i,t} \\Sigma_{i=1}^I |w_{i,t}|) , & \\text{if }t \\in B \\\\\n",
    "\\Delta p_{i,t}, & \\text{otherwise} \\end{cases} $$\n",
    "\n",
    "$$\\delta_{i,t} = \\begin{cases}\n",
    "p_{i,t}-o_{i,t}, & \\text{if }t-1 \\in B \\\\\n",
    "\\Delta p_{i,t}, & \\text{otherwise} \\end{cases} $$\n",
    "\n",
    "$$K_t = K_{t-1} + \\Sigma_{i=1}^I h_{i,t-1} \\psi_{i,t}(\\delta_{i,t} + d_{i,t})$$ ,\n",
    "\n",
    "$K_0=1$, $h_{i,t}$: The holidngs(number of securities or contracts) of instrument i at time t.\n",
    "         $\\delta_{i,t}$: The change of Market value between $t-1$ and $t$ for instrument i.\n",
    "         \n",
    "- PnL are always reinvested whenever $t \\subseteq B$: No negative value.\n",
    "- $d_{i,t}$ are already embedded in $K_t$: No need for the strategy.\n",
    "- The purpose of $w_{i,t} (\\Sigma_{i=1}^I |w_{i,t}|)^{-1}$: To de-lever the allocations.\n",
    "- May not know $p_{i,t}$ of the new contract at a roll time t: Use $o_{i,t+1}$ as the closest in time.\n",
    "\n",
    "Let $\\tau_i$ be the transaction cost associated with trading 1 dollar of instrument i.\n",
    "\n",
    "1. Rebalance Costs: ${{c_t}}$\n",
    "   - $c_t = \\Sigma_{i=1}^I (|h_{i,t-1}| p_{i,t} + |h_{i,t}| o_{i,t+1}) \\psi_{i,t} \\tau_{i}, \\forall t \\in B$\n",
    "   \n",
    "\n",
    "2. Bid-ask spread: $\\tilde{c_t}$\n",
    "   - $\\tilde{c_t} = \\Sigma_{i=1}^I |h_{i,t-1}| p_{i,t} \\psi_{i,t} \\tau_{i,t}$\n",
    "   \n",
    "3. Volume: $v_t$\n",
    "   - $v_t = \\underset{i}{\\operatorname{min}}$ $v_{i,t} \\over |h_{i,t-1}|$\n",
    "   \n",
    "\n",
    "Transaction costs functions are not necessarily linear. Thanks to the ETF trick, we can model a basket of futures(or a sing futures) as if it was a single non-expiring cash product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 PCA Weights (주성분 분석 가중치)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to derive the vector ${W_t}$.  \n",
    "\n",
    "Consider an IID multivariate Gaussian process characterized by a vector of means $\\mu$, of size $NxL$, and a covariance matrix $v$, of size $NxN$(stochastic process). We would like to compute the vector of allocation $\\omega$ that conforms to a particular distribution of risks accross $v$`s principal components.\n",
    "\n",
    "1. We perform a spectral decomposition, $VW = W\\Lambda$.\n",
    "\n",
    "\n",
    "2. Given a vector of allocations $\\omega$, we can compute the portfolio`s risk as \n",
    "\n",
    "\n",
    "   $$\\sigma ^2 = \\omega'W\\Lambda W'\\omega = \\beta'\\Lambda\\beta = (\\Lambda^{1/2}\\beta)(\\Lambda^{1/2}\\beta)$$ \n",
    "   - where $\\beta$: The projection of $\\omega$ on the orthogonal basis.\n",
    "\n",
    "\n",
    "3. $\\Lambda$: A diagonal matrix, $\\sigma^2 = \\Sigma_{n=1}^N\\beta_n^2\\Lambda_{n,n}$,\n",
    "\n",
    "\n",
    "   The risk atrributed to the $n$th component: $R_n$,\n",
    "   $$R_n = \\beta_n^2\\Lambda_{n,n}\\sigma^{-2} = [W'\\omega]_n^2\\Lambda_{n,n}\\sigma^2$$\n",
    "   - with $R'1_n=1$, and $1_n=$ a vector of $N$ ones.\n",
    "   - We can interpret $({R_n})_{n=1,...,N}$ as the distribution of risks across the orthogonal components.\n",
    "   \n",
    "4. We would like to compute the vector $\\omega$ that delievers a user-defined risk distribution $R$.\n",
    "   - $\\beta = (\\sigma\\sqrt{R_n/\\Lambda_{n,n}})_{n=1,...,N}$: The allocation in the new(orthogonal) basis\n",
    "   \n",
    "5. The allocation in the old basis is given by $\\omega = W\\beta$(The risk distribution constant).\n",
    "   - Figure 2.2 illustrates the contribution to risk per principla component for an invers variance allocation.\n",
    "   - For the PCA portfolio, only the component with lowest variance contributes risk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](Image/Figure2.2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Snippet 2.1 PCA Weights from a Risk Distribution R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcaWeights(cov, riskDist=None, riskTarget=1.):\n",
    "    # Following the riskAlloc distribution, match riskTarget\n",
    "    eVal, eVec=np.linalg.eigh(cov) # must be Hermitian\n",
    "    indices=eVal.argsort()[::-1] # arguments for sorting eVal desc\n",
    "    eVal, eVec=eVal[indices], eVec[:,indices]\n",
    "    if riskDist is None:\n",
    "        riskDist=np.zeros(cov.shape[0])\n",
    "        riskDist[-1]=1\n",
    "    loads=riskTarget*(riskDist/eVal)**.5\n",
    "    wghts=np.dot(eVec,np.reshape(loads,(-1,1)))\n",
    "    #ctr=(loads/riskTarget)**2*eVal # verify riskDist\n",
    "    return wghts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.4.3 Single Future Roll\n",
    "\n",
    "When dealing with a single futures contract, an equivalent and more direct approach is to form a time series of cumulative roll gaps, and detract that gaps series from the price series. \n",
    "- FUT_CUR_GEN_TICKER: It identifies the contract associated with that price. Its value changes with every roll.\n",
    "- PX_OPEN: The open price associated with that bar.\n",
    "- PX_LAST: The close price associated with the bar.\n",
    "- VWAP: The volume-weighted average price associated with that bar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Snippet 2.2 Form a Gap Series, Detract it from Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRolledSeries(pathIn, key):\n",
    "    series=pd.read_hdf(pathIn, key='bars/ES_10k')\n",
    "    series['Time']=pd.to_datetime(series['Time'],format='%Y%m%d%H%M%S%f')\n",
    "    series=series.set_index('Time')\n",
    "    gaps=rollGaps(series)\n",
    "    for fld in ['Close', 'VWAP']:series[fld] -=gaps\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollGaps(series,dictio={'Instrument':'FUT_CUR_GEN_TICKER','Open':'PX_OPEN', 'Close':'PX_LAST'},matchEnd=True):\n",
    "    # Compute gaps at each roll, between previous close and next open\n",
    "    rollDates=series[dictio['instrument']].drop_duplicates(keep='first').index\n",
    "    gaps=series[dictio['Close']]*0\n",
    "    iloc=list(series.index)\n",
    "    iloc=[iloc.index(i)-1 for i in rollDates] # index of days prior to roll\n",
    "    gaps.loc[rollDates[1:]]=series[dictio['Open']].loc[rollDates[1:]]-series[dictio['Close']].iloc[iloc[1:]].values\n",
    "    gaps=gaps.cumsum()\n",
    "    if matchEnd:gaps-=gaps.iloc[-1] # roll backward\n",
    "    return gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rolled prices are used for simulating PnL and portfolio mark-to-market values.\n",
    "- However, raw prices should still be used to size positions and determine capital consumption.\n",
    "\n",
    "##### Non-Negative Rolled Series\n",
    "\n",
    "1. Compute a tiem series of rolled futures prices\n",
    "2. Compute the return(r) as rolled price change divided by the previous raw price.\n",
    "3. Form a price series using those returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Snippet 2.3 Non-Negative Rolled Price Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filePath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5ab7abf29dbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mraw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilePath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mgaps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrollGaps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdictio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Instrument'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'Symbol'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Open'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'Open'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Close'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'Close'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrolled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfld\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Open'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Close'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrolled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfld\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m\u001b[0mgaps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrolled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Returns'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrolled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Close'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Close'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filePath' is not defined"
     ]
    }
   ],
   "source": [
    "raw=pd.read_csv(filePath,index_col=0,parse_dates=True)\n",
    "gaps=rollGaps(raw,dictio={'Instrument':'Symbol','Open':'Open','Close':'Close'})\n",
    "rolled=raw.copy(deep=True)\n",
    "for fld in ['Open', 'Close']:rolled[fld] -=gaps\n",
    "rolled['Returns']=rolled['Close'].diff()/raw['Close'].shift(1)\n",
    "rolled['rPrices']=(1+rolled['Returns']).cumprod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Sampling Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weakness of Sampling Features\n",
    "\n",
    "1. Several ML algorithms do not scale well with sample size(e.g. SVMs).\n",
    "2. ML algorithms achieve highest accuracy when they attempt to learn from relevant examples.\n",
    "   - We discuss ways of sampling bars to produce a features matrix with relevant training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Sampling for Reduction\n",
    "\n",
    "One reason for sampling features from a structured dataset is to reduce the amount of data used to fit the ML algorithm.\n",
    "- Downsampling \n",
    "  - Linspace sampling: Sequential sampling at a constant size\n",
    "  - Uniform sampling: Sampling randomly using a uniform distribution\n",
    "- The sample does not necessarily contain the subset of most relevant observations in terms of their predictive power or informational content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Event-Based Sampling \n",
    "\n",
    "Portfolio managers typically place a bet after some event takes place(a structural break, an extracted signal, microstructural phenomena). We can characterize an event as significant, and let the ML algorithm learn whether there is an accurate prediction function under those circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2.1 The CUSUM Filter\n",
    "\n",
    "The CUSUM filter is a quality-control method, designed to detect a shift in the mean value of a measured quantity away from a target value. Consider IID observations $(y_t)_{t=1,...,T}$ arising form a locally stationary process.\n",
    "\n",
    "1. Define the cumulative sums $S_t = {\\operatorname{max}}(0, S_{t-1}+y_t-E_{t-1}[y_t])$ with boundary condition $S_0=0$.\n",
    "   - Recommend an action at the first $t$ satisfying $S_t \\in h$, for some threshold h(the filter size).\n",
    "\n",
    "   \n",
    "2. Note $S_t=0$ whenever $y_t \\le E_{t-1}[y_t]-S_{t-1}$. This zero floor means that we will skip some downward deviations that otherwise would make $S_t$ negative. \n",
    "   - The filter is set up to identify a sequence of upside divergence from any reset level zero.\n",
    "   - The threshold is activated when $$S_t \\ge h \\Leftrightarrow \\exists_{\\tau}\\in[1,t]|\\Sigma_{i=\\tau}^t(y_t-E_{i-1}[y_t]) \\ge h$$\n",
    "   \n",
    "3. CUSUM filter:\n",
    "$$S_t^+ = \\operatorname{max}(0, S_{t-1}^+ + y_t - E_{t-1}[y_t]), S_0^+=0$$\n",
    "$$S_t^- = \\operatorname{max}(0, S_{t-1}^- + y_t - E_{t-1}[y_t]), S_0^-=0$$\n",
    "$$S_t = \\operatorname{max}(S_t^+,-S_t^-)$$\n",
    "\n",
    "   - We will sample a bar $t$ if and only if $S_t \\ge h$, at which point $S_t$ is reset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Snippet 2.4 The Symmetric CUSUM Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTEvents(gRaw,h):\n",
    "    tEvents, sPos, sNeg=[], 0, 0\n",
    "    diff=gRaw.diff()\n",
    "    for i in diff.index[1:]:\n",
    "        sPos, sNeg=max(0, sPos+diff.loc[i]), min(0, sNeg+diff.loc[i])\n",
    "        if sNeg < -h:\n",
    "            sNeg=0; tEvents.append(i)\n",
    "        elif sPos > h:\n",
    "            sPos=0; tEvents.append(i)\n",
    "    return pd.DatetimeIndex(tEvents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One practical aspect that makes CUSUM filters appealing is that multiple events are not triggered by $gRaw$ hovering around a threshold level, a flaw suffered by popular market signals such as Bollinger bands. Variable '$S_t$' could be based on any of the features like structural break statistics, enthropy, or market microstructure measurements. Once we have obtained this subset of event-driven bars, we will let the ML algorithm determine whether the occurence of such events constitutes actionable intelligence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_64",
   "language": "python",
   "name": "py37_64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
